## Stable Diffusion: From Text to Images

### Overview

In this module, we will take an in-depth look at what Stable Diffusion is, how it works, and how to integrate it into an API. We'll write code that can take a textual prompt and generate an image that matches the description.

### What is PyTorch?

PyTorch or `torch` is an open-source machine learning library for Python developed by Facebook's AI Research lab (FAIR). It's widely used for deep learning and artificial intelligence applications. PyTorch is known for its flexibility, ease of use, and dynamic computation graph, making it a favorite among researchers and developers alike.

### What is Stable Diffusion?

Stable Diffusion is a technique for generating images based on textual descriptions. It starts with an initial image (typically random noise) and evolves it over time, with each step guided by a model trained to match the data distribution. This iterative process eventually leads to the creation of an image that closely aligns with the textual prompt.

### Implementing Stable Diffusion

The implementation of Stable Diffusion involves setting up a pipeline that takes in a textual prompt and outputs a generated image. For this, we'll use the `DiffusionPipeline` from the `diffusers` library. The `diffusers` library uses PyTorch under the hood.

Let's see an example of how we can implement this with a class named `ImageKit`:

File: `image_kit.py`

```python
import os
from diffusers import DiffusionPipeline


class ImageKit:
    directory = "jobs"
    if not os.path.exists(directory):
        os.makedirs(directory)

    def __call__(self,
                 job_id: str,
                 prompt: str,
                 negative_prompt: str,
                 model_id: str,
                 height: int,
                 width: int,
                 guidance_scale: float,
                 num_inference_steps: int):
        self.pipeline = DiffusionPipeline.from_pretrained(model_id)
        image, *_ = self.pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
        ).images
        image.save(os.path.join(self.directory, f"{job_id}.png"))

```

In this example, the `ImageKit` class uses the `DiffusionPipeline` to perform the Stable Diffusion process. There are several input parameters, let's take a look at each one and see how they affect the output image.

- `job_id`: This is a unique identifier for each job. It's typically generated by the system and used to track the progress of each job and to associate the results (in this case, the generated image) with the job. This ID is also used to retrieve the job details or results at a later point in time.
- `prompt`: This is the textual description that guides the image generation process. The Stable Diffusion model will generate an image that corresponds to this text. For instance, a prompt could be "a scenic mountain range at sunset" or "a group of penguins on an iceberg". For more information about sophisticated prompting, lookup "Prompt Engineering" in your favorite search engine or LLM.
- `negative_prompt`: This is an optional textual description that guides the image generation process in the opposite direction. It tells the Stable Diffusion model what we do not want to appear in the generated image. For instance, if the `prompt` is "a scenic mountain range" and the `negative_prompt` is "snow", the model will generate an image of a mountain range with the intent to exclude snow from the scene. This is a powerful way to fine-tune the output of the image generation process, allowing you to explicitly specify elements you want to avoid in the generated image. Please note that the effectiveness of a negative prompt can vary based on the model and the specific prompts used.
- `model_id`: This identifies the specific pre-trained model to use for the Stable Diffusion process. Different models could have been trained on different datasets or using different settings, so the choice of model could drastically affect the characteristics of the generated image. Here are just a few of the models... Checkout the text-to-image models on [huggingface](https://huggingface.co/models?pipeline_tag=text-to-image&sort=trending) for more.
    - stabilityai/stable-diffusion-2-1
    - SG161222/Realistic_Vision_V1.4
    - runwayml/stable-diffusion-v1-5
- `height` & `width`: These parameters define the dimensions of the output image. The model will generate an image with the specified height and width, measured in pixels.
- `guidance_scale`: This parameter controls the influence of the guidance on the diffusion process. The guidance is derived from the input prompt and helps steer the random diffusion process towards generating an image that matches the prompt. A higher `guidance_scale` makes the output image adhere more closely to the prompt.
- `num_inference_steps`: This parameter controls the number of steps in the diffusion process. Each step updates the image based on the guidance and the current state of the image. More steps could lead to a more refined image, but would also take more time to compute.

These parameters together control the image generation process. By adjusting these parameters, you can influence the characteristics of the output image, including its appearance, quality, and the time it takes to generate it.

## Example Usage

```python
image_kit = ImageKit()
image_kit(
    job_id="test-001",
    prompt="Photograph of an astronaut riding a horse on Mars",
    negative_prompt="cartoon",
    model_id="stabilityai/stable-diffusion-2-1",
    height=512,
    width=512,
    guidance_scale=7.5,
    num_inference_steps=20,
)
```

### Using Stable Diffusion in Our FastAPI App

We can use the Stable Diffusion technique to add an exciting new feature to our FastAPI application: turning textual prompts into images. To do this, we'll need two endpoints for our application:

1. A `POST` endpoint at `/jobs` that accepts a textual prompt and other parameters needed for the image generation job. This endpoint will initiate the job and return a job ID that the user can use to retrieve the resulting image once it's ready.

2. A `GET` endpoint at `/jobs/{job_id}` that accepts a job ID and returns the image generated by the job with that ID, if it's ready.

Q: Wait, why two endpoints? Can't we just return the image?

A: We need two endpoints because stable diffusion can take several minutes to complete and most servers will throw a timeout error after 30-45 seconds. To avoid this we'll set up a background task. This means we can have our endpoint return immediately while it works in the background to generate an image. This means that our API can handle new requests while it's working in the background.

File: `main.py`

```python
import os
from uuid import uuid4
from fastapi import FastAPI
from fastapi.responses import FileResponse
from fastapi.background import BackgroundTasks
from fastapi.exceptions import HTTPException
from image_kit import ImageKit

app = FastAPI()
image_kit = ImageKit()


@app.post("/jobs")
async def post_job(queue: BackgroundTasks,
                   prompt: str,
                   negative_prompt: str,
                   model_id: str = "stabilityai/stable-diffusion-2-1",
                   height: int = 512,
                   width: int = 512,
                   guidance_scale: float = 7.5,
                   num_inference_steps: int = 100):
    job_id = uuid4()
    parameters = dict(
        job_id=job_id,
        prompt=prompt,
        negative_prompt=negative_prompt,
        model_id=model_id,
        height=height,
        width=width,
        guidance_scale=guidance_scale,
        num_inference_steps=num_inference_steps,
    )
    queue.add_task(image_kit, **parameters)
    return {"job_id": job_id}


@app.get("/jobs/{job_id}")
async def get_job(job_id: str):
    filepath = os.path.join("jobs", f"{job_id}.png")
    if os.path.exists(filepath):
        return FileResponse(filepath, media_type="image/png")
    else:
        raise HTTPException(404, "Work in Progress")

```

In the next module, we will learn about MongoDB and how to add CRUD operations to our API, enhancing its functionality and making it more robust. Stay tuned!

### Challenge
1. Build your own Stable Diffusion API, but this time use Pydantic to validate incoming data.
